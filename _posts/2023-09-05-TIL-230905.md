---
categories: [TIL]
tags: [til, deep learning, python]
img_path: /assets/lib/post-img/
mermaid: true
math: true
---

# Deep Learning

- [Teachable Machine](https://teachablemachine.withgoogle.com/train)
- 컴퓨터가 우리 뇌의 일부와 유사한 방식으로 학습하는 방법론

## 딥러닝 모델

- 많은 수의 인공 뉴런으로 이루어진 네트워크
- `층`이라고 하는 여러 단계를 거쳐 데이터를 입력받아 출력

### `Sigmoid`

- 딥러닝과 머신러닝에서 자주 사용되는 활성화 함수
  - `binary classification`(이진 분류)에 주로 사용
    - 마지막 `출력층`의 활성화 함수로 사용
- 실숫값을 입력으로 받아 `0-1` 사이의 값으로 압축

  $$
  S(x)= \frac{1}{1 + e^{-x}}
  $$

  - e는 자연 상수

#### `Sigmoid` 함수를 사용하는 이유

- 값의 범위가 제한적
  - 출력이 `0-1`의 사이이기 때문에 출력값을 일정한 범위 내로 조절 가능
  - 확률을 표현할 때 유용
- 비선형성
  - 비선형 함수라 모델이 복잡한 패턴 학습 가능

#### `Sigmoid` 함수의 형태

- `S` 모양의 곡선
  ![Sigmoid Function](230905/4.png)
- `X` 값이 매우 크면 `1`에 가까워지고, `X` 값이 매우 작으면 `0`에 가까워짐
- `X = 0` 일 때는 `0.5`

#### `Sigmoid` 함수의 주의점

- `Vanishing Gradient` (그래디언트 소실, 기울기 소실)
  - `X` 값이 너무 크거나 작을 때, 함수의 기울기(미분값)가 매우 작아져 모델이 제대로 학습되지 않는 문제점

#### `Sigmoid` 함수 구현하기

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid_f(x):
  return 1 / (1 + np.exp(-x))

# sigmoid 함수 그리기
x = np.arange(-5.0 , 5.0, 0.1)
y = sigmoid_f(x)

plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![sigmoid](230905/5.png)

#### `Sigmoid` 함수 미분하기

$$
\frac{d}{dx}sigmoid(x) = sigmoid(x)(1 - sigmoid(x))
$$

```python
def sigmoid_d(x):
  return sigmoid_f(x) * (1 - sigmoid_f(x))

y = sigmoid_d(x)

plt.plot(x, y)
plt.ylim(-0.1, 1.0)
plt.show()

y.max()
```

![sigmoid 미분](230905/6.png)

```bash
0.25
```

- 이렇게 미분하다 보면 `0`에 가까워져 학습이 어려워짐

### `tanh` 함수

- `Sigmoid` 함수의 단점 개선

```python
y = np.tanh(x)

plt.plot(x, y)
plt.show()
```

### `tanh` 함수 미분하기

$$
\frac{d}{dx}tanh(x) = 1 - tanh^2(x) = (1 - tanh(x))(1 + tanh(x))
$$

![tanh](230905/7.png)

```python
def tanh_d(x):
  return (1-np.tanh(x))*(1+np.tanh(x))

y = tanh_d(x)

plt.plot(x, y)
plt.show()

y.max()
```

![tanh 미분하기](230905/8.png)

- 그런데 얘도 미분하다 보면 `0`에 가까워짐

### ReLU

- `0`보다 작으면 `0`, `0`보다 크면 `x` 값 할당

```python
def relu_f(x):
  return np.maximum(0, x)

y = relu_f(x)

plt.plot(x, y)
plt.show()
```

![ReLU](230905/9.png)

- 미분하면 `0`일 때는 0, 그 외에는 `1`이기 때문에 `기울기 소실` 발생시키지 않음
- `음수값`이라면?
  - 음수가 나오면 해당 노드는 학습 안 됨
- 활성화 함수로 `RuLU`를 사용하더라도 마지막 Layer에는 `Sigmoid`나 `tanh` 함수 사용을 통해 `0-1` 사이의 값을 나타내 정확히 분류

### `Activation Function` 활성화 함수

- 신경망은 선형회귀와 달리 한 계층의 신호를 다음 계층으로 그대로 전달하지 않고 비선형적인 활성화 함수를 거친 후 전달
- 생물학적인 신경망 모방한 것
  - 약한 신호는 전달하지 않고 너무 강한 신호도 전달하지 않는 `S`자 형 곡선과 같이 `비선형적` 반응하는 것

## 손 글씨 데이터셋을 이용한 다진 분류

![딥러링 과정](230905/1.png)

- 오차가 큰 이유 -> `가중치`
- `가중치` 조절 필요
- `sigmoid` 학습 안 됨 -> 정확도 똑같이 나옴 == 기울기 소실

### 데이터 다운로드

```python
from tensorflow.keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train.shape, y_train.shape, X_test.shape, y_test.shape
```

- 손 글씨 숫자 데이터셋
  - 훈련 데이터 6만 개, 테스트 데이터 만개
  - 각각의 데이터는 28 x 28 크기로 구성
  - 흑백 이미지(0-255)로 구성
  - 배경은 검은색, 글씨가 흰색으로 구성

### 데이터 하나 확인

```python
import matplotlib.pyplot as plt

digit = X_train[0]
label = y_train[0]

plt.imshow(digit, cmap = "gray")
plt.show()

print(label)
```

### 데이터 전처리

- 2차원 데이터를 1차원 데이터로 변환 (DNN 층의 입력이 1차원이므로)
- 데이터값의 범위를 0-255에서 0-1로 변환 (정규화(Scaling)) -> 과적합 줄어듦

### 2차원 -> 1차원

```python
X_train = X_train.reshape(60000, 28*28)
X_test = X_test.reshape(10000, 28*28)
```

### 스케일링

```python
X_train = X_train.astype("float32") / 255
X_test = X_test.astype("float32") / 255

X_train.shape, X_test.shape
```

```bash
((60000, 784), (10000, 784))
```

### y 원핫 인코딩

```python
from tensorflow.keras.utils import to_categorical

y_train_en = to_categorical(y_train)
y_test_en = to_categorical(y_test)

y_train_en.shape, y_test_en.shape
```

```bash
((60000, 10), (10000, 10))
```

### 라벨값 확인

```python
import pandas as pd

print(pd.Series(y_train).unique())
print(pd.Series(y_test).unique())
```

```bash
[5 0 4 1 9 2 3 6 7 8]
[7 2 1 0 4 9 5 6 3 8]
```

### 모델 설계

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model1 = Sequential()

# 입력층
# model1.add(Dense(units = 512, input_dim = 28 * 28, activation = "sigmoid"))
model1.add(Dense(units = 512, input_dim = X_train.shape[1], activation = "sigmoid"))

# 은닉층
model1.add(Dense(units = 256, activation = "sigmoid"))

# 출력층
# model1.add(Dense(units = 10, activation = "softmax"))
model1.add(Dense(units = y_train_en.shape[1], activation = "softmax"))

model1.summary()
```

```bash
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 512)               401920

 dense_1 (Dense)             (None, 256)               131328

 dense_2 (Dense)             (None, 10)                2570

=================================================================
Total params: 535,818
Trainable params: 535,818
Non-trainable params: 0
_________________________________________________________________
```

### 컴파일

```python
model1.compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = ["accuracy"])
```

### 학습

- batch_size

  - 한번에 학습할 때 사용할 데이터의 수

- validation_data
  - 검증데이터 셋 설정

```python
h1 = model1.fit(X_train, y_train_en, epochs = 10,
                batch_size = 100, validation_data=(X_test, y_test_en))
```

```bash
Epoch 1/10
600/600 [==============================] - 6s 9ms/step - loss: 0.4640 - accuracy: 0.8718 - val_loss: 0.2393 - val_accuracy: 0.9290
Epoch 2/10
600/600 [==============================] - 4s 7ms/step - loss: 0.2021 - accuracy: 0.9395 - val_loss: 0.1662 - val_accuracy: 0.9507
Epoch 3/10
600/600 [==============================] - 4s 7ms/step - loss: 0.1436 - accuracy: 0.9574 - val_loss: 0.1219 - val_accuracy: 0.9631
Epoch 4/10
600/600 [==============================] - 5s 9ms/step - loss: 0.1064 - accuracy: 0.9688 - val_loss: 0.1071 - val_accuracy: 0.9663
Epoch 5/10
600/600 [==============================] - 4s 7ms/step - loss: 0.0818 - accuracy: 0.9750 - val_loss: 0.0885 - val_accuracy: 0.9719
Epoch 6/10
600/600 [==============================] - 4s 7ms/step - loss: 0.0633 - accuracy: 0.9807 - val_loss: 0.0825 - val_accuracy: 0.9735
Epoch 7/10
600/600 [==============================] - 5s 9ms/step - loss: 0.0503 - accuracy: 0.9845 - val_loss: 0.0704 - val_accuracy: 0.9780
Epoch 8/10
600/600 [==============================] - 4s 7ms/step - loss: 0.0399 - accuracy: 0.9877 - val_loss: 0.0702 - val_accuracy: 0.9773
Epoch 9/10
600/600 [==============================] - 5s 8ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0717 - val_accuracy: 0.9774
Epoch 10/10
600/600 [==============================] - 5s 8ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.0720 - val_accuracy: 0.9788
```

### 시각화

```python
import matplotlib.pyplot as plt
import numpy as np

ep = np.arange(1, 11)

# 정확도
plt.plot(ep, h1.history["accuracy"], color = "blue", label = "train")
plt.plot(ep, h1.history["val_accuracy"], color = "green", label = "test")
plt.legend()
plt.grid()
plt.show()

# loss
plt.plot(ep, h1.history["loss"], color = "blue", label = "train")
plt.plot(ep, h1.history["val_loss"], color = "green", label = "test")
plt.legend()
plt.grid()
plt.show()
```

![accuracy](230905/2.png)
![loss](230905/3.png)

### 예측하기

```python
pred = model1.predict(X_test[125:126])

print(pred)
print(y_test[125:126])
```

```bash
1/1 [==============================] - 0s 22ms/step
[[1.0989827e-07 6.4058913e-06 3.5534991e-04 2.6660701e-04 3.7173466e-03
  6.7704332e-07 8.6490132e-08 8.2146589e-06 4.1975020e-04 9.9522543e-01]]
[9]
```

### 직접 쓴 손 글씨 숫자 인식하기

```python
# 이미지 가져오기
import PIL.Image as pilimg

gray_img = pilimg.open("./Data/3.png").convert("L")

# 파이썬 이미지 배열로 변환
pix = np.array(gray_img)

pix.shape
```

```bash
(28, 28)
```

### 전처리

```python
new_img = pix.reshape(1, 28 * 28)
new_img = new_img.astype("float32") / 255

new_img.shape
```

```bash
(1, 784)
```

### 예측 모델에 넣기

```python
pred = model1.predict(new_img)

pred
```

```bash
1/1 [==============================] - 0s 26ms/step
[[1.0989827e-07 6.4058913e-06 3.5534991e-04 2.6660701e-04 3.7173466e-03
  6.7704332e-07 8.6490132e-08 8.2146589e-06 4.1975020e-04 9.9522543e-01]]
```

### 배열 중 가장 큰 값 확인하기

```python
np.argmax(pred)
```

```bash
9
```
